{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298a2001",
   "metadata": {},
   "source": [
    "Basic MLP that employs a forward propagation then back propagation algorithm.\n",
    "Works as follows:\n",
    "\n",
    "Input layer corresponding to size of data being fed in\n",
    "\n",
    "Hidden layer\n",
    "\n",
    "Output layer of one node\n",
    "\n",
    "-Initially randomise weights and set biases to -1\n",
    "\n",
    "-Apply forward propagation algorithm using a choice of activation - linear is what is used\n",
    "\n",
    "-Calculate difference between target variable and output\n",
    "\n",
    "-Apply backpropagation algorithm and update weights\n",
    "\n",
    "-Add one to epoch, save weights and start over\n",
    "\n",
    "-At a selected Epoch mutliply weights by some factor\n",
    "\n",
    "-At end, visualise the training with minimisation and weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c729a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(BaseEstimator, ClassifierMixin): \n",
    "    def __init__(self, params=None):     \n",
    "        if (params == None):\n",
    "            self.inputLayer = 10                       # Input Layer\n",
    "            self.hiddenLayer = 50                      # Hidden Layer\n",
    "            self.OutputLayer = 1                       # Outpuy Layer\n",
    "            self.learningRate = 0.001                  # Learning rate\n",
    "            self.max_epochs = 50                      # Epochs\n",
    "            self.BiasHiddenValue = -1                   # Bias HiddenLayer initial\n",
    "            self.BiasOutputValue = -1                  # Bias OutputLayer initial\n",
    "            self.activ = self.activation['linear'] # Activation function\n",
    "            self.deriv = self.derivative['linear']\n",
    "            self.perturn_weights = False\n",
    "        else:\n",
    "            self.inputLayer = params['InputLayer']\n",
    "            self.hiddenLayer = params['HiddenLayer']\n",
    "            self.OutputLayer = params['OutputLayer']\n",
    "            self.learningRate = params['LearningRate']\n",
    "            self.max_epochs = params['Epochs']\n",
    "            self.BiasHiddenValue = params['BiasHiddenValue']\n",
    "            self.BiasOutputValue = params['BiasOutputValue']\n",
    "            self.activ = self.activation[params['ActivationFunction']]\n",
    "            self.deriv = self.derivative[params['ActivationFunction']]\n",
    "            self.perturn_weights = params['Epochs']\n",
    "        \n",
    "        'Starting Bias and Weights'\n",
    "        self.WEIGHT_hidden = self.starting_weights(self.hiddenLayer, self.inputLayer)\n",
    "        self.WEIGHT_output = self.starting_weights(self.OutputLayer, self.hiddenLayer)\n",
    "        self.BIAS_hidden = np.array([self.BiasHiddenValue for i in range(self.hiddenLayer)])\n",
    "        self.BIAS_output = np.array([self.BiasOutputValue for i in range(self.OutputLayer)])\n",
    "        self.output_number = 1 #defines how many numbers are in the output\n",
    "        \n",
    "        \n",
    "    pass\n",
    "    \n",
    "    def starting_weights(self, x, y):\n",
    "        return [[2  * random.random() - 1 for i in range(x)] for j in range(y)]\n",
    "\n",
    "    activation = {\n",
    "         'sigmoid': (lambda x: 1/(1 + np.exp(-x))),\n",
    "            'tanh': (lambda x: np.tanh(x)),\n",
    "            'Relu': (lambda x: x*(x > 0)),\n",
    "            'linear': (lambda x: x)\n",
    "               }\n",
    "    derivative = {\n",
    "         'sigmoid': (lambda x: x*(1-x)),\n",
    "            'tanh': (lambda x: 1-x**2),\n",
    "            'Relu': (lambda x: 1 * (x>0)),\n",
    "            'linear': (lambda x: np.ones_like(x))\n",
    "               }\n",
    " \n",
    "    def Backpropagation_Algorithm(self, inputs, ERROR_output):\n",
    "\n",
    "        'Stage 1 - Error: OutputLayer'\n",
    "        DELTA_output = ((-1)*(ERROR_output) * self.deriv(self.OUTPUT_L2))\n",
    "\n",
    "        'Stage 2 - Update weights OutputLayer and HiddenLayer'\n",
    "        for i in range(self.hiddenLayer):\n",
    "            for j in range(self.OutputLayer):\n",
    "                self.WEIGHT_output[i][j] -= (self.learningRate * (DELTA_output[j] * self.OUTPUT_L1[i]))\n",
    "                self.BIAS_output[j] -= (self.learningRate * DELTA_output[j])\n",
    "          \n",
    "        'Stage 3 - Error: HiddenLayer'\n",
    "        delta_hidden = np.matmul(self.WEIGHT_output, DELTA_output)* self.deriv(self.OUTPUT_L1)\n",
    "\n",
    "        'Stage 4 - Update weights HiddenLayer and InputLayer(x)'\n",
    "        for i in range(self.OutputLayer):\n",
    "            for j in range(self.hiddenLayer):\n",
    "                self.WEIGHT_hidden[i][j] -= (self.learningRate * (delta_hidden[j] * inputs[i]))\n",
    "                self.BIAS_hidden[j] -= (self.learningRate * delta_hidden[j])\n",
    "                \n",
    "    def show_err_graphic(self,error,epochs):\n",
    "        plt.figure(figsize=(9,4))\n",
    "        plt.plot(epochs, error, \"m-\",color=\"b\", marker=11)\n",
    "        plt.xlabel(\"Number of Epochs\")\n",
    "        plt.ylabel(\"Absolute Error \");\n",
    "        plt.title(\"Error Minimization\")\n",
    "        plt.show()\n",
    "\n",
    "    def visualise_weights(self,weights):\n",
    "        layer_idx = 0  # Index of the layer you want to visualize\n",
    "        weights = weights[layer_idx]  # Weights matrix for the selected layer\n",
    "        \n",
    "        # Reshape the weights matrix to a 2D grid\n",
    "        weights= np.array(weights)\n",
    "        grid_weights = weights.reshape(self.hiddenLayer, self.inputLayer)  # Assuming visualising hidden layer\n",
    "        \n",
    "        # Create a figure and axis\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        \n",
    "        # Plot the weights as an image\n",
    "        im = ax.imshow(grid_weights, cmap='viridis')  # Choose a colormap\n",
    "        \n",
    "        # Add colorbar for mapping values to colors\n",
    "        cbar = ax.figure.colorbar(im, ax=ax)\n",
    "        cbar.ax.set_ylabel('Weight Values', rotation=-90, va=\"bottom\")\n",
    "        \n",
    "        # Set title and labels\n",
    "        ax.set_title(f'Layer {layer_idx} Weights Visualization')\n",
    "        ax.set_xlabel('Input Neuron Index')\n",
    "        ax.set_ylabel('Hidden Neuron Index')\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    \n",
    "    def predict(self, X, y):\n",
    "        'Returns the predictions for every element of X'\n",
    "        my_predictions = []\n",
    "        'Forward Propagation'\n",
    "        forward = np.matmul(X,self.WEIGHT_hidden) + self.BIAS_hidden\n",
    "        forward = np.matmul(forward, self.WEIGHT_output) + self.BIAS_output\n",
    "                                 \n",
    "        for i in forward:\n",
    "            my_predictions.append(i)\n",
    "            \n",
    "        array_score = []\n",
    "        for i in range(len(my_predictions)):\n",
    "            error = y[i]-my_predictions[i]\n",
    "            #Check if the prediction is close to the actual\n",
    "            if np.abs(error) <0.05:\n",
    "                correct = 1\n",
    "            else:\n",
    "                correct=0\n",
    "            array_score.append(correct)\n",
    "            \n",
    "        total_accuracy = (sum(array_score) / len(array_score))*100\n",
    "        print(\"Total Accuracy:\", total_accuracy)\n",
    "\n",
    "        return my_predictions, total_accuracy\n",
    "\n",
    "    def fit(self, X, y):  \n",
    "        count_epoch = 1\n",
    "\n",
    "        epoch_array = []\n",
    "        error_array = []\n",
    "        W0 = []\n",
    "        W1 = []\n",
    "        while(count_epoch <= self.max_epochs):\n",
    "            for inputs, target in zip(X,y): \n",
    "                'Stage 1 - (Forward Propagation)'\n",
    "                self.OUTPUT_L1 = self.activ((np.dot(inputs, self.WEIGHT_hidden) + self.BIAS_hidden.T))\n",
    "                self.OUTPUT_L2 = self.activ((np.dot(self.OUTPUT_L1, self.WEIGHT_output) + self.BIAS_output.T))\n",
    "\n",
    "                #Find the difference between the correct datapoint and the NN output\n",
    "                total_error = target - self.OUTPUT_L2\n",
    "\n",
    "                'Backpropagation : Update Weights'\n",
    "                self.Backpropagation_Algorithm(inputs, total_error)\n",
    "                \n",
    "            if((count_epoch % 50 == 0)or(count_epoch == 1)):\n",
    "                print(\"Epoch \", count_epoch, \"- Total Error: \",total_error)\n",
    "\n",
    "                error_array.append(total_error)\n",
    "                epoch_array.append(count_epoch)\n",
    "            #Perturb the weights on epoch 13 to test.\n",
    "            if (count_epoch== 13) and self.perturn_weights:\n",
    "                print(\"Perturbing weights by 1.3\")\n",
    "                if count_epoch == 13 and self.perturn_weights:\n",
    "                    for row_index, row in enumerate(self.WEIGHT_hidden):\n",
    "                        for col_index, value in enumerate(row):\n",
    "                            try:\n",
    "                                self.WEIGHT_hidden[row_index][col_index] *= 1.3\n",
    "                            except TypeError:\n",
    "                                print(\"Non-numeric value found in self.WEIGHT_hidden.\")\n",
    "\n",
    "            W0.append(self.WEIGHT_hidden)\n",
    "            W1.append(self.WEIGHT_output)\n",
    "             \n",
    "            count_epoch += 1\n",
    "        #Show loss minimisation\n",
    "        self.show_err_graphic(error_array,epoch_array)\n",
    "        \n",
    "        #Show heatmap of weights\n",
    "        self.visualise_weights(W0)\n",
    "        \"\"\"\n",
    "        plt.plot(W0[0])\n",
    "        plt.title('Weight Hidden update during training')\n",
    "        plt.legend(['neuron1', 'neuron2', 'neuron3', 'neuron4', 'neuron5'])\n",
    "        plt.ylabel('Value Weight')\n",
    "        plt.show()\n",
    "        \"\"\"\n",
    "        plt.plot(W1[0])\n",
    "        plt.title('Weight Output update during training')\n",
    "        plt.legend(['neuron1'])\n",
    "        plt.ylabel('Value Weight')\n",
    "        plt.show()\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a1f968",
   "metadata": {},
   "source": [
    "Currently generating data via both gaussian and interval spacing between 0 and 1. Target is std of gaussian and mutliplication of the step number respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9072607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_generate_data_gaussian(num_samples):\n",
    "    y_train = []\n",
    "    x_train = []\n",
    "    mean =  0\n",
    "    x_values = np.linspace(-3, 3, 10)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        #standard_deviation = random.uniform(0.5, 1.5) #Produces a random float value\n",
    "        standard_deviation = random.randrange(1,5) #Produces a integer between the range\n",
    "        #gaussian_vector = gaussian(x_values, mean, standard_deviation)\n",
    "        rv = norm(loc = mean, scale = standard_deviation)\n",
    "        gaussian_vector = rv.pdf(x_values)\n",
    "        y_train.append([standard_deviation])  # Append standard_deviation as a new row\n",
    "        x_train.append(list(gaussian_vector))  # Append gaussian vector as a new row\n",
    "    #plt.plot(x_values,gaussian_vector)\n",
    "    return np.array(x_train), np.array(y_train)\n",
    "    \n",
    "\n",
    "def randomly_generate_data_uniform(num_samples):\n",
    "    y_train = []\n",
    "    x_train = []\n",
    "    x_values = np.linspace(0, 1, 10)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        #standard_deviation = random.uniform(0.5, 1.5) #Produces a random float value\n",
    "        multi_num = random.randrange(1,8) #Produces a integer between the range\n",
    "        numbers = multi_num*x_values\n",
    "\n",
    "        y_train.append([multi_num])  # Append standard_deviation as a new row\n",
    "        x_train.append(list(numbers))  # Append gaussian vector as a new row\n",
    "    #plt.plot(x_values,gaussian_vector)\n",
    "    return np.array(x_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc16a6",
   "metadata": {},
   "source": [
    "Generate data and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26d5ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(471)  \n",
    "x_train, y_train = randomly_generate_data_uniform(1000)\n",
    "x_test, y_test = randomly_generate_data_uniform(100)\n",
    "#%%\n",
    "Perceptron = MultiLayerPerceptron()\n",
    "Perceptron.fit(x_train,y_train)\n",
    "#%%\n",
    "predictions, accuracy = Perceptron.predict(x_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
